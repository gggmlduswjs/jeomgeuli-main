import os, time, threading
import google.generativeai as genai
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
from google.api_core.exceptions import DeadlineExceeded, ServiceUnavailable

MODEL_NAME = os.environ.get("GEMINI_MODEL", "gemini-1.5-flash")  # í•„ìš”ì‹œ proë¡œ êµì²´

class TransientError(RuntimeError): ...
class RateLimitError(RuntimeError): ...

SYSTEM_PROMPT = (
    "ë‹¹ì‹ ì€ í•œêµ­ì–´ë¡œ ë‹µí•˜ëŠ” ìœ ìš©í•œ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. "
    "ì§ˆë¬¸ì´ ëª¨í˜¸í•´ë„ í•©ë¦¬ì ì¸ ê°€ì •ì„ ë°íˆê³  ë¨¼ì € ìš”ì•½Â·í•µì‹¬ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤. "
    "ê·¸ ë‹¤ìŒì— í•„ìš”í•œ ì¶”ê°€ ì •ë³´ë¥¼ 1ë¬¸ì¥ìœ¼ë¡œ ì •ì¤‘íˆ ë¬¼ì–´ë´…ë‹ˆë‹¤. "
    "ë‹µë³€ì€ ë¶ˆí•„ìš”í•œ ì‚¬ì¡± ì—†ì´ 5~8ì¤„ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”."
)

def _get_api_key():
    key = os.environ.get("GOOGLE_API_KEY")
    if not key:
        raise RuntimeError("GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤(.env ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ í™•ì¸).")
    return key

def _get_model():
    genai.configure(api_key=_get_api_key())
    return genai.GenerativeModel(
        MODEL_NAME,
        system_instruction=SYSTEM_PROMPT,
        safety_settings={
            # í•„ìš” ì‹œ ì •ì±… ë§ê²Œ ì¡°ì •
            "HARASSMENT": "BLOCK_ONLY_HIGH",
            "HATE": "BLOCK_ONLY_HIGH",
        },
        generation_config={
            "temperature": 0.7,
            "top_p": 0.9,
            "max_output_tokens": 512,
        },
    )

def explain_gemini_error(e: Exception) -> str:
    msg = str(e)
    # í”í•œ ì¼€ì´ìŠ¤ ë§¤í•‘
    if "permission" in msg.lower():
        return "API ê¶Œí•œ/ë¹Œë§ ë¬¸ì œë¡œ ìš”ì²­ì´ ê±°ë¶€ë˜ì—ˆìŠµë‹ˆë‹¤."
    if "quota" in msg.lower() or "rate" in msg.lower() or "429" in msg:
        return "ìš”ì²­ì´ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    if "Invalid resource name" in msg or "model not found" in msg.lower():
        return "ëª¨ë¸ ì´ë¦„ì´ ì˜ëª»ë˜ì—ˆê±°ë‚˜ ì ‘ê·¼ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤."
    if "GOOGLE_API_KEY" in msg or "not set" in msg.lower():
        return "ì„œë²„ì— GOOGLE_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤. .env/í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ê³  ì„œë²„ë¥¼ ì¬ì‹œì‘í•˜ì„¸ìš”."
    return f"LLM ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {msg}"

# ğŸ”’ 429(ë ˆì´íŠ¸ë¦¬ë°‹)ì€ ì¬ì‹œë„ ê¸ˆì§€. ë„¤íŠ¸ì›Œí¬/ì„œë²„ ë¶ˆì•ˆì •(503/timeout)ë§Œ ì¬ì‹œë„.
@retry(
  reraise=True,
  stop=stop_after_attempt(2),
  wait=wait_exponential(multiplier=1, max=8),
  retry=retry_if_exception_type((DeadlineExceeded, ServiceUnavailable, TimeoutError))
)
def generate_reply(query: str, history: list[dict] | None = None) -> str:
    """
    history ì˜ˆì‹œ: [{"role":"user","content":"..."},{"role":"assistant","content":"..."}]
    """
    # â³ í”„ë¡œì„¸ìŠ¤ ë‚´ ì†Œí”„íŠ¸ ë ˆì´íŠ¸ë¦¬ë°‹(ë²„ìŠ¤íŠ¸ ë°©ì§€)
    _MIN_GAP = float(os.getenv("LLM_MIN_GAP_SEC", "0.8"))  # í˜¸ì¶œ ìµœì†Œ ê°„ê²©(ì´ˆ)
    if not hasattr(generate_reply, "_gate"):
        generate_reply._gate = {"ts": 0.0, "lock": threading.Lock()}
    with generate_reply._gate["lock"]:
        now = time.time()
        wait = generate_reply._gate["ts"] + _MIN_GAP - now
        if wait > 0:
            time.sleep(wait)
        generate_reply._gate["ts"] = time.time()
    
    # í‚¤/ëª¨ë¸ ì¤€ë¹„ ì‹¤íŒ¨ ì‹œ ì¦‰ì‹œ ì˜ˆì™¸ (ì¬ì‹œë„ ì—†ìŒ)
    model = _get_model()
    # Geminiì˜ ë©€í‹°í„´ ëŒ€í™” í¬ë§·ìœ¼ë¡œ ë³€í™˜
    chat_history = []
    for m in (history or []):
        role = "user" if m.get("role") == "user" else "model"
        chat_history.append({"role": role, "parts": [m.get("content", "")]})

    chat = model.start_chat(history=chat_history)
    try:
        resp = chat.send_message(query)
    except Exception as e:
        # ì˜ˆì™¸ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ëŒì´ ì½ëŠ” RuntimeError ë©”ì‹œì§€ë¡œ ë³€í™˜
        raise RuntimeError(explain_gemini_error(e))
    
    # resp.textê°€ ì—†ìœ¼ë©´ candidates/partsì—ì„œ ìˆ˜ë™ ì¶”ì¶œ
    text = getattr(resp, "text", None)
    if not text:
        try:
            text = resp.candidates[0].content.parts[0].text  # type: ignore
        except Exception:
            text = ""
    return text.strip()
